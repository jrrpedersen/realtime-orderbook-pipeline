# Realtime Orderbook Pipeline

[![Phase](https://img.shields.io/badge/Phase-3%20Monitoring-blue.svg)](#)
[![Python](https://img.shields.io/badge/Python-3.11%2B-blue.svg)](#)
[![Kafka](https://img.shields.io/badge/Kafka-Streaming-black.svg)](#)
[![Docker](https://img.shields.io/badge/Docker-Compose-blue.svg)](#)
[![Async](https://img.shields.io/badge/Async-asyncio-purple.svg)](#)
[![Data](https://img.shields.io/badge/Data-Streaming-orange.svg)](#)
[![Observability](https://img.shields.io/badge/Observability-Prometheus%20%2B%20Grafana-orange.svg)](#)
[![Parquet](https://img.shields.io/badge/Parquet-Cold_Store-orange.svg)](#)

> **Category:** Data Engineering Â· Streaming Systems Â· Trading Infrastructure

A **production-inspired real-time data pipeline** for ingesting, validating, storing, monitoring, and auditing high-frequency orderbook data.

This project demonstrates how to build a **trustworthy data foundation** for trading and analytics systems, with explicit attention to data quality, fault tolerance, observability, and hot vs. cold storage separation.

---

## ğŸ§© Problem Statement

Modern trading systems rely on data that is:
- always available
- timely
- internally consistent
- auditable after the fact

This project models a realistic data foundation for such systems:
- real-time ingestion of orderbook events
- streaming validation safeguards
- low-latency â€œhotâ€ storage
- immutable â€œcoldâ€ historical storage
- batch-level data quality audits
- full system observability

---

## ğŸ— High-Level Architecture

```
            +-------------------+
            |  Ingestion Service |
            |  (Python, Kafka)  |
            +---------+---------+
                      |
                      v
                Kafka (raw ticks)
                      |
                      v
            +-------------------+
            |  Quality Service  |
            |  - schema checks |
            |  - domain rules  |
            +----+---------+---+
                 |         |
                 |         |
                 v         v
         Postgres (Hot)   Parquet Lake (Cold)
             |                 |
             |                 v
             |         Batch Quality Audits
             |       (Great Expectations)
             |
     Dashboards / Queries

Monitoring:
Ingestion + Quality services â†’ Prometheus â†’ Grafana
```

---

## ğŸ“¦ Project Structure

```
realtime-orderbook-pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion_service/
â”‚   â”œâ”€â”€ quality_service/
â”‚   â”œâ”€â”€ data_audit/
â”‚   â””â”€â”€ common/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ lake/
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ prometheus/
â”‚   â””â”€â”€ grafana/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ Phase3_Observability.md
â”‚   â””â”€â”€ Phase4_Cold_Storage_and_GX.md
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## ğŸ” Pipeline Phases

### Phase 1 â€“ Ingestion & Streaming
- Simulated high-frequency orderbook ticks
- Kafka as a durable, decoupled buffer

### Phase 2 â€“ Streaming Validation & Hot Storage
- Schema and domain validation
- Valid data persisted to Postgres

### Phase 3 â€“ Observability
- Prometheus metrics exposed by services
- Grafana dashboards for system health

### Phase 4 â€“ Cold Storage & Batch Audits
- Parquet-based data lake
- Offline Great Expectations audits

---

## ğŸ”¥ Hot vs â„ï¸ Cold Storage

| Layer | Purpose | Technology |
|-----|-------|------------|
| Hot store | Low-latency queries | Postgres |
| Cold store | Historical analytics | Parquet |

Data is written to hot and cold stores independently after validation.

---

## âœ… Data Quality Strategy

**Streaming (real-time):**
- Enforced in the quality service

**Batch (offline):**
- Great Expectations over Parquet data

---

## ğŸ“Š Observability

Each service exposes Prometheus metrics.
Grafana provides real-time dashboards for ingestion and validation throughput.

---

## ğŸ§ª Running the Project Locally

```bash
docker-compose up -d
```

```bash
cd src
python -m ingestion_service.main
```

```bash
cd src
python -m quality_service.main
```

```bash
python -m data_audit.audit_parquet --symbol TTF-GAS --date 2025-12-09
```

---

## ğŸ§  Design Decisions

- Kafka for reliable streaming
- Parquet for long-term storage
- Explicit separation of streaming and batch validation
- Version-pinned data tooling

---

## ğŸš€ Future Extensions

- Parquet compaction
- Retention policies
- Schema evolution
- Flink-based processing


---

## Technology Stack / Tags

Python Â· Kafka Â· Streaming Data Â· Market Data Â· Orderbook Â· Data Engineering Â· Event-Driven Architecture Â· Async IO Â· Docker Â· Trading Systems Â· Prometheus Â· Grafana